
df = spark.read.json("/mnt/c/Users/miles.MILE-BL-4766-LA.000/Documents/FT/futurense_hadoop-pyspark/labs/dataset/people/people.json")

df.show()
df.printSchema()

df.select("name").show()

df.select(df["name"], df["age"] + 1).show()
df.filter(df["age"] > 21).show()
df.groupBy("age").count().show()

#df.registerTempTable("people")
df.createOrReplaceTempView("people")

teenagers = sqlContext.sql("SELECT name, age FROM people WHERE age >= 13 AND age <= 19")
teenagers.show()	// Displays records
teenagers.count()	// Displays count

teenagers.collect()	// Collects data from Dataframe to Array
teenagers.collect().foreach(println)	// Prints data available in the Dataframe

#Write data in parquet format to hdfs
teenagers.select("name", "age").write.format("parquet").save("/home/ubuntu/spark/parquet")

#Write data in parquet format to local file system
teenagers.select("name", "age").write.format("parquet").save("file:///home/ubuntu/spark/teenagers.parquet")

#Read the data in parquet from hdfs
newDf = spark.read.format("parquet").load("/home/ubuntu/spark/parquet")

#Print details
newDf.show()

#Write data in orc format to hdfs 
newDf.select("name", "age").write.format("orc").save("/home/ubuntu/spark/orc/teenagers.orc")

#Start Spark Shell with Avro dependency
pyspark --packages org.apache.spark:spark-avro_2.12:3.0.1

#Read the data in parquet from hdfs
newDf = spark.read.format("parquet").load("/home/ubuntu/spark/parquet")

#Write data in avro format
newDf.select("name", "age").write.format("avro").save("/home/ubuntu/spark/avro/teenagers.avro")

#Read data in avro format
spark.read.format("avro").load("/home/ubuntu/spark/avro/teenagers.avro")



spark.sql("select count(*) as count, \
case when age < 13 then 'kids' \
when age < 20 then 'teenagers' \
when age < 31 then 'young' \
when age < 50 then 'middle' \
else 'Seniors' \
end \
as cat from people where age is not null group by cat").show()